# Movies-ETL
Module 8 - ETL

##Overview
In this module, we learned how to use the Extract, Transform, Load (ETL) process to create data pipelines. A data pipeline moves data from a source to a destination, and the ETL process creates data pipelines that also transform the data along the way.

We gathered data from both Wikipedia and Kaggle, combined them, and saved them into a SQL database so that hackathon participants would have a nice, clean dataset to use. To do this, we followed the ETL process: extract the Wikipedia and Kaggle data from their respective files, transform the datasets by cleaning them up and joining them together, and load the cleaned dataset into a SQL database.

